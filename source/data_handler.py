# data_handler.py
"""
Handles loading, preprocessing, calculating indicators, and splitting of time series data.
"""
import pandas as pd
import numpy as np
import warnings
import pandas_ta as ta # Import pandas-ta

# Import config to access indicator parameters if needed directly,
# but primarily using names defined there
from config import SMA_PERIODS, EMA_PERIODS, RSI_PERIOD, ATR_PERIOD, \
                   BBANDS_PERIOD, BBANDS_STD_DEV, MACD_FAST, MACD_SLOW, MACD_SIGNAL, \
                   INDICATOR_FEATURES # Import the list of expected indicator names

def calculate_indicators(df):
    """Appends technical indicators to the DataFrame."""
    print("Calculating technical indicators...")
    if df is None or df.empty:
        return df

    # Ensure required columns exist
    required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
    if not all(col in df.columns for col in required_cols):
        raise ValueError("DataFrame missing required OHLCV columns for TA calculation.")

    # Use pandas_ta DataFrame extension
    # Calculate indicators - arguments match those defined in config.py
    # SMA
    for period in SMA_PERIODS:
        df.ta.sma(length=period, append=True)
    # EMA
    for period in EMA_PERIODS:
        df.ta.ema(length=period, append=True)
    # RSI
    df.ta.rsi(length=RSI_PERIOD, append=True)
    # ATR
    df.ta.atr(length=ATR_PERIOD, append=True)
    # Bollinger Bands
    df.ta.bbands(length=BBANDS_PERIOD, std=BBANDS_STD_DEV, append=True)
    # MACD
    df.ta.macd(fast=MACD_FAST, slow=MACD_SLOW, signal=MACD_SIGNAL, append=True)

    print(f"Columns after TA calculation: {df.columns.tolist()}")
    # Check if all expected indicator columns were created
    missing_ta_cols = [col for col in INDICATOR_FEATURES if col not in df.columns]
    if missing_ta_cols:
        warnings.warn(f"Warning: The following expected indicator columns were not generated by pandas-ta: {missing_ta_cols}")
        # Consider raising an error or handling this based on requirements

    return df

def load_and_split_data(file_path, train_ratio=0.6, val_ratio=0.2):
    """
    Loads data, calculates indicators and returns, and splits chronologically.

    Args:
        file_path (str): Path to the CSV file.
        train_ratio (float): Proportion of data for the training set.
        val_ratio (float): Proportion of data for the validation set.

    Returns:
        tuple: (train_data, val_data, test_data) pandas DataFrames.
    """
    try:
        data = pd.read_csv(file_path)
        print(f"Loaded raw data: {data.shape[0]} rows")
    except FileNotFoundError:
        print(f"Error: Data file not found at {file_path}")
        return None, None, None

    # Basic Preprocessing
    try:
        data['Open time'] = pd.to_datetime(data['Open time'])
        data['Close time'] = pd.to_datetime(data['Close time'])
        data.sort_values(by='Open time', inplace=True) # Ensure chronological order EARLY
    except Exception as e:
        warnings.warn(f"Date parsing warning (non-critical): {e}")

    # --- Calculate Technical Indicators ---
    try:
        data = calculate_indicators(data)
    except Exception as e:
        print(f"Error calculating technical indicators: {e}")
        # Decide how to handle: return None or continue without indicators?
        # Returning None is safer if indicators are essential.
        return None, None, None

    # --- Calculate Target Variable (Return) ---
    # Must be done AFTER sorting and potentially indicator calculation
    # if indicators could affect row count/order, though usually done on OHLCV.
    data['Return'] = data['Close'].shift(-1) / data['Close'] - 1

    # --- Handle NaNs ---
    # Handle NaNs generated by indicators (at the start) AND the final NaN from 'Return' calculation.
    # Also handle any NaNs potentially present in the original OHLCV data.
    initial_len = len(data)
    # Define ALL columns that must be valid (base features + target + indicators)
    # Use the FEATURES list from config + 'Return'
    from config import FEATURES # Import FEATURES list defined in config
    essential_cols_for_training = FEATURES + ['Return']
    # Ensure only columns that actually exist in the dataframe are checked
    cols_to_check = [col for col in essential_cols_for_training if col in data.columns]

    data.dropna(subset=cols_to_check, inplace=True)
    rows_dropped = initial_len - len(data)
    if rows_dropped > 0:
        print(f"Dropped {rows_dropped} rows due to NaNs in features, indicators, or target.")

    if len(data) < 100: # Check again after dropping NaNs
        print("Error: Not enough data remaining after calculating indicators and dropping NaNs.")
        return None, None, None

    # --- Chronological Splitting ---
    n = len(data)
    train_end_idx = int(n * train_ratio)
    val_end_idx = int(n * (train_ratio + val_ratio))

    train_data = data.iloc[:train_end_idx].copy()
    val_data = data.iloc[train_end_idx:val_end_idx].copy()
    test_data = data.iloc[val_end_idx:].copy()

    print(f"Data Split: Train={len(train_data)}, Validation={len(val_data)}, Test={len(test_data)}")

    if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
        print("Error: One or more data splits are empty after processing. Check ratios, data length, and NaN handling.")
        return None, None, None

    return train_data, val_data, test_data